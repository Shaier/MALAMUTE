{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2ZryqM3SCD1"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip /content/openstax_probing_dataset_polish.zip\n",
        "!unzip /content/openstax_probing_dataset_spanish.zip"
      ],
      "metadata": {
        "id": "GofwRluf5NFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "\n",
        "def md5_hash(string):\n",
        "    string_bytes = string.encode('utf-8')\n",
        "    md5_hash = hashlib.md5()\n",
        "    md5_hash.update(string_bytes)\n",
        "    md5_hash_hex = md5_hash.hexdigest()\n",
        "    return md5_hash_hex"
      ],
      "metadata": {
        "id": "pX59TOb05Ygf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "directory = '/content/content/Spanish_v5'\n",
        "\n",
        "id_to_json_line = {}\n",
        "\n",
        "unique_ids = set()\n",
        "\n",
        "for filename in tqdm(os.listdir(directory)):\n",
        "    if filename.endswith('.json'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        with open(file_path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "            for k in data.keys():\n",
        "              for level in data[k].keys():\n",
        "                for prompt in data[k][level]:\n",
        "                  unique_id = md5_hash(prompt['prompt'] + prompt['label'])\n",
        "\n",
        "                  if unique_id not in unique_ids:\n",
        "                    unique_ids.add(unique_id)\n",
        "\n",
        "                    id_to_json_line[unique_id] = {\"custom_id\": \"gpt-4o-mini_spanish_\" + unique_id,\n",
        "                            \"method\": \"POST\",\n",
        "                            \"url\": \"/v1/chat/completions\",\n",
        "                            \"body\": {\"model\": \"gpt-4o-mini\",\n",
        "                                    \"messages\": [{\"role\": \"system\",\n",
        "                                                  \"content\": \"You are a helpful, respectful and honest assistant. Your answers should be crisp, short and not repetitive. Here are 5 examples: Example 1: Prompt: Francesco Bartolomeo Conti was born in [MASK]. Answer: Florence. Example 2: Prompt: English bulldog is a subclass of [MASK]. Answer: dog. Example 3: Prompt: The official language of Mauritius is [MASK]. Answer: English. Example 4: Prompt: Nicotine binds to [MASK]. Answer: CHRNA4. Example 5: Prompt: Hepatitis has symptoms such as [MASK]. Answer: Abdominal pain.\"},\n",
        "                                                    {\"role\": \"user\", \"content\": f\"Prompt: {prompt['prompt']}\"}],\"max_tokens\": 1000}}\n",
        "\n",
        "\n",
        "directory = '/content/content/Polish_v5'\n",
        "for filename in tqdm(os.listdir(directory)):\n",
        "    if filename.endswith('.json'):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        with open(file_path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "            for k in data.keys():\n",
        "              for level in data[k].keys():\n",
        "                for prompt in data[k][level]:\n",
        "                  unique_id = md5_hash(prompt['prompt'] + prompt['label'])\n",
        "\n",
        "                  if unique_id not in unique_ids:\n",
        "                    unique_ids.add(unique_id)\n",
        "\n",
        "                    id_to_json_line[unique_id] = {\"custom_id\": \"gpt-4o-mini_polish_\" + unique_id,\n",
        "                            \"method\": \"POST\",\n",
        "                            \"url\": \"/v1/chat/completions\",\n",
        "                            \"body\": {\"model\": \"gpt-4o-mini\",\n",
        "                                    \"messages\": [{\"role\": \"system\",\n",
        "                                                  \"content\": \"You are a helpful, respectful and honest assistant. Your answers should be crisp, short and not repetitive. Here are 5 examples: Example 1: Prompt: Francesco Bartolomeo Conti was born in [MASK]. Answer: Florence. Example 2: Prompt: English bulldog is a subclass of [MASK]. Answer: dog. Example 3: Prompt: The official language of Mauritius is [MASK]. Answer: English. Example 4: Prompt: Nicotine binds to [MASK]. Answer: CHRNA4. Example 5: Prompt: Hepatitis has symptoms such as [MASK]. Answer: Abdominal pain.\"},\n",
        "                                                    {\"role\": \"user\", \"content\": f\"Prompt: {prompt['prompt']}\"}],\"max_tokens\": 1000}}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GDIT6Ml62F2",
        "outputId": "99f0c944-826e-4108-f9cb-6c20ffed069a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 25.27it/s]\n",
            "100%|██████████| 6/6 [00:00<00:00, 43.23it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(id_to_json_line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2nf_j8aCoDP",
        "outputId": "a58f1508-2368-4418-a894-1bf016eb5a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16561"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"gpt-4o-mini-multilingual-full.json\", 'w') as f:\n",
        "  for k in id_to_json_line.keys():\n",
        "    f.write(json.dumps(id_to_json_line[k]) + '\\n')"
      ],
      "metadata": {
        "id": "4wFRVZY4CpDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "dZFWJyIlCuIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"gpt-4o-mini-multilingual-full.json\"\n",
        "\n",
        "batch_input_file = client.files.create(\n",
        "  file=open(file_path, \"rb\"),\n",
        "  purpose=\"batch\"\n",
        ")\n",
        "\n",
        "batch_input_file_id = batch_input_file.id\n",
        "\n",
        "\n",
        "client.batches.create(\n",
        "    input_file_id=batch_input_file_id,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\",\n",
        "    metadata={\n",
        "\n",
        "      \"description\": \"gpt-4o-mini-multilingual-full\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mnzmlfvDYfu",
        "outputId": "993c421b-a05f-4307-fd35-a904307abe19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Batch(id='batch_670d951005e0819094fddfad4502fd5a', completion_window='24h', created_at=1728943376, endpoint='/v1/chat/completions', input_file_id='file-rB3bufM9q8XEgaN73v8gzuAf', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1729029776, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'gpt-4o-mini-multilingual-full'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def split_jsonl_file(file_path, num_splits=5):\n",
        "    with open(file_path, 'r') as infile:\n",
        "        lines = infile.readlines()\n",
        "\n",
        "    total_lines = len(lines)\n",
        "    lines_per_split = total_lines // num_splits\n",
        "    remainder = total_lines % num_splits\n",
        "\n",
        "    start = 0\n",
        "    for i in range(num_splits):\n",
        "        end = start + lines_per_split + (1 if i < remainder else 0)\n",
        "\n",
        "        split_file_path = f'gpt-4-turbo-multilingual-split_{i+1}.jsonl'\n",
        "        with open(split_file_path, 'w') as outfile:\n",
        "            outfile.writelines([l.replace('gpt-4o-mini', 'gpt-4-turbo-2024-04-09') for l in lines[start:end]])\n",
        "\n",
        "        print(f\"Split {i+1} written to {split_file_path}\")\n",
        "\n",
        "        start = end\n"
      ],
      "metadata": {
        "id": "BvgAuh2_DjpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "split_jsonl_file('/content/gpt-4o-mini-multilingual-full.json', num_splits=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSJ7qaTiIxZ_",
        "outputId": "46e81664-6121-49bb-98a3-5958ce02cf0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 1 written to gpt-4-turbo-multilingual-split_1.jsonl\n",
            "Split 2 written to gpt-4-turbo-multilingual-split_2.jsonl\n",
            "Split 3 written to gpt-4-turbo-multilingual-split_3.jsonl\n",
            "Split 4 written to gpt-4-turbo-multilingual-split_4.jsonl\n",
            "Split 5 written to gpt-4-turbo-multilingual-split_5.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/gpt-4-turbo-multilingual-split_5.jsonl\"\n",
        "\n",
        "batch_input_file = client.files.create(\n",
        "  file=open(file_path, \"rb\"),\n",
        "  purpose=\"batch\"\n",
        ")\n",
        "\n",
        "batch_input_file_id = batch_input_file.id\n",
        "\n",
        "\n",
        "client.batches.create(\n",
        "    input_file_id=batch_input_file_id,\n",
        "    endpoint=\"/v1/chat/completions\",\n",
        "    completion_window=\"24h\",\n",
        "    metadata={\n",
        "\n",
        "      \"description\": \"gpt-4-turbo-multilingual-split_5\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wST6wWDiIzA9",
        "outputId": "e34acaaf-3cce-4889-cbc3-af4c1ff37aa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Batch(id='batch_670dbbdb06108190bdecc247db191eef', completion_window='24h', created_at=1728953307, endpoint='/v1/chat/completions', input_file_id='file-iBbQCiF1S8TfY1yQBUUMOMLs', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1729039707, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'gpt-4-turbo-multilingual-split_5'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dicts = []\n",
        "statuses = []\n",
        "metadatum = []\n",
        "\n",
        "for data in client.batches.list():\n",
        "  data_dict = data.to_dict()\n",
        "\n",
        "  if data_dict['status'] not in ['failed', 'cancelled', 'cancelling']:\n",
        "    data_dicts.append(data_dict)\n",
        "    statuses.append(data_dict['status'])\n",
        "    metadatum.append(data_dict['metadata'])\n",
        "\n",
        "batches_that_have_run = list(set([m['description'][len(\"gpt-4o-mini-multilingual\") + 1:]+'.jsonl' for m in metadatum if \"gpt-4o-mini-multilingual\" in m['description']]))"
      ],
      "metadata": {
        "id": "TLlW-W_GL9HW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "340e7f47-9f4f-4f6d-846f-81aed0457ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-bbc8dc89e55f>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmetadatum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;31m# by pydantic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_T\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_page_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36miter_pages\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_next_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                 \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mget_next_page\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_to_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_api_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request_api_list\u001b[0;34m(self, model, page, options)\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    955\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             response = self._client.send(\n\u001b[0m\u001b[1;32m    991\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    924\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    927\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    955\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    989\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m         )\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    197\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    114\u001b[0m                 trace.return_value = (\n\u001b[1;32m    115\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    225\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1286\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts =[]\n",
        "\n",
        "for data in client.batches.list():\n",
        "  data_dict = data.to_dict()\n",
        "  if data_dict['status'] not in ['failed', 'cancelled', 'cancelling'] and 'gpt-4o-mini-multilingual' in data_dict['metadata']['description']:\n",
        "\n",
        "    file_response = client.files.content(data_dict['output_file_id'])\n",
        "\n",
        "    texts.append(file_response.text)"
      ],
      "metadata": {
        "id": "5sHQua1Qq41l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoEuUKWirRPt",
        "outputId": "56d8de76-f035-4956-9a2a-992441db0419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "id_to_pred_gpt_4o_mini = {}\n",
        "\n",
        "for text in tqdm(texts):\n",
        "  lines = text.split('\\n')\n",
        "\n",
        "  for line in lines[:-1]:\n",
        "    x = json.loads(line)\n",
        "    id_to_pred_gpt_4o_mini[x['custom_id'].split('_')[-1]] = x['response']['body']['choices'][0]['message']['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQgAVtTkrFnF",
        "outputId": "d02d8523-abe8-428f-e685-c28ea42658bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  4.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(id_to_pred_gpt_4o_mini)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qzu4rhTrXNO",
        "outputId": "f43d0e0a-b145-4302-e71b-20dda59fa057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16561"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "\n",
        "def md5_hash(string):\n",
        "    string_bytes = string.encode('utf-8')\n",
        "    md5_hash = hashlib.md5()\n",
        "    md5_hash.update(string_bytes)\n",
        "    md5_hash_hex = md5_hash.hexdigest()\n",
        "    return md5_hash_hex"
      ],
      "metadata": {
        "id": "_icBhhEOsk25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-4o-mini"
      ],
      "metadata": {
        "id": "J6OuB_QA924k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def best_subspan_score(label, completion):\n",
        "  if label.lower() in completion.lower():\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "directory = \"/content/content/Spanish_v5\"\n",
        "\n",
        "\n",
        "total_pg = []\n",
        "total_st = []\n",
        "\n",
        "for filename in tqdm(os.listdir(directory)):\n",
        "    if filename.endswith('.json'):\n",
        "        scores_pg = []\n",
        "        scores_st = []\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for k in data.keys():\n",
        "              for level in data[k].keys():\n",
        "                for prompt in data[k][level]:\n",
        "                  unique_id = md5_hash(prompt['prompt'] + prompt['label'])\n",
        "\n",
        "                  pred = id_to_pred_gpt_4o_mini[unique_id]\n",
        "\n",
        "                  score = best_subspan_score(prompt['label'], pred)\n",
        "\n",
        "                  assert level in [\"paragraph_level\", \"sentence_level\"]\n",
        "                  if level == \"paragraph_level\":\n",
        "                    scores_pg.append(score)\n",
        "                  else:\n",
        "                    scores_st.append(score)\n",
        "        print(f\"{filename}: pg_level: {sum(scores_pg)/len(scores_pg)}, st_level: {sum(scores_st)/len(scores_st)}\")\n",
        "        total_pg.extend(scores_pg)\n",
        "        total_st.extend(scores_st)\n",
        "\n",
        "\n",
        "print(\"GPT-4o-mini Spanish\")\n",
        "\n",
        "\n",
        "print()\n",
        "print(f\"Spanish total: pg_level: {sum(total_pg)/len(total_pg)}, st_level: {sum(total_st)/len(total_st)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayJtGjmxruFs",
        "outputId": "d50b058b-c4dc-474a-c2bf-25dff60b20ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 109.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prec%C3%A1lculo-2ed.json: pg_level: 0.3498312710911136, st_level: 0.2576419213973799\n",
            "c%C3%A1lculo-volumen-2.json: pg_level: 0.26666666666666666, st_level: 0.1717171717171717\n",
            "introducci%C3%B3n-estad%C3%ADstica-empresarial.json: pg_level: 0.41, st_level: 0.36231884057971014\n",
            "qu%C3%ADmica-2ed.json: pg_level: 0.5104166666666666, st_level: 0.3923739237392374\n",
            "c%C3%A1lculo-volumen-3.json: pg_level: 0.25609756097560976, st_level: 0.20647773279352227\n",
            "f%C3%ADsica-universitaria-volumen-3.json: pg_level: 0.4190800681431005, st_level: 0.3224043715846995\n",
            "c%C3%A1lculo-volumen-1.json: pg_level: 0.3249097472924188, st_level: 0.2579185520361991\n",
            "f%C3%ADsica-universitaria-volumen-2.json: pg_level: 0.4333958724202627, st_level: 0.29471544715447157\n",
            "f%C3%ADsica-universitaria-volumen-1.json: pg_level: 0.39065108514190316, st_level: 0.2730496453900709\n",
            "introducci%C3%B3n-estad%C3%ADstica.json: pg_level: 0.3739495798319328, st_level: 0.32460732984293195\n",
            "GPT-4o-mini Spanish\n",
            "\n",
            "Spanish total: pg_level: 0.3962637362637363, st_level: 0.301119023397762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def best_subspan_score(label, completion):\n",
        "  if label.lower() in completion.lower():\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "directory = \"/content/content/Polish_v5\"\n",
        "\n",
        "total_pg = []\n",
        "total_st = []\n",
        "\n",
        "for filename in tqdm(os.listdir(directory)):\n",
        "    if filename.endswith('.json'):\n",
        "        scores_pg = []\n",
        "        scores_st = []\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for k in data.keys():\n",
        "              for level in data[k].keys():\n",
        "                for prompt in data[k][level]:\n",
        "                  unique_id = md5_hash(prompt['prompt'] + prompt['label'])\n",
        "\n",
        "                  pred = id_to_pred_gpt_4o_mini[unique_id]\n",
        "\n",
        "                  score = best_subspan_score(prompt['label'], pred)\n",
        "\n",
        "                  assert level in [\"paragraph_level\", \"sentence_level\"]\n",
        "                  if level == \"paragraph_level\":\n",
        "                    scores_pg.append(score)\n",
        "                  else:\n",
        "                    scores_st.append(score)\n",
        "        print(f\"{filename}: pg_level: {sum(scores_pg)/len(scores_pg)}, st_level: {sum(scores_st)/len(scores_st)}\")\n",
        "        total_pg.extend(scores_pg)\n",
        "        total_st.extend(scores_st)\n",
        "\n",
        "\n",
        "print(\"GPT-4o-mini Polish\")\n",
        "\n",
        "\n",
        "print()\n",
        "print(f\"Spanish total: pg_level: {sum(total_pg)/len(total_pg)}, st_level: {sum(total_st)/len(total_st)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRFzrmV3t9bk",
        "outputId": "aa7752f5-b493-4b12-fc73-b40ca3590a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00, 53.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fizyka-dla-szk%C3%B3%C5%82-wy%C5%BCszych-tom-3.json: pg_level: 0.23283082077051925, st_level: 0.1903914590747331\n",
            "psychologia-polska.json: pg_level: 0.3238993710691824, st_level: 0.2001361470388019\n",
            "fizyka-dla-szk%C3%B3%C5%82-wy%C5%BCszych-tom-2.json: pg_level: 0.2578947368421053, st_level: 0.16977611940298507\n",
            "mikroekonomia-podstawy.json: pg_level: 0.30183727034120733, st_level: 0.1856763925729443\n",
            "fizyka-dla-szk%C3%B3%C5%82-wy%C5%BCszych-tom-1.json: pg_level: 0.21833333333333332, st_level: 0.14564831261101244\n",
            "makroekonomia-podstawy.json: pg_level: 0.28634361233480177, st_level: 0.20089285714285715\n",
            "GPT-4o-mini Polish\n",
            "\n",
            "Spanish total: pg_level: 0.28077290076335876, st_level: 0.18558786346396966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qgenA_nCuXCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-4o"
      ],
      "metadata": {
        "id": "-FbzjMSdunE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts =[]\n",
        "\n",
        "for data in client.batches.list():\n",
        "  data_dict = data.to_dict()\n",
        "  if data_dict['status'] not in ['failed', 'cancelled', 'cancelling'] and 'gpt-4o-multilingual' in data_dict['metadata']['description']:\n",
        "\n",
        "    file_response = client.files.content(data_dict['output_file_id'])\n",
        "\n",
        "    texts.append(file_response.text)"
      ],
      "metadata": {
        "id": "zkZArZMduom4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "id_to_pred_gpt_4o = {}\n",
        "\n",
        "for text in tqdm(texts):\n",
        "  lines = text.split('\\n')\n",
        "\n",
        "  for line in lines[:-1]:\n",
        "    x = json.loads(line)\n",
        "    id_to_pred_gpt_4o[x['custom_id'].split('_')[-1]] = x['response']['body']['choices'][0]['message']['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw0GDaI7uqzl",
        "outputId": "c02baf1c-bd01-460b-8517-ed4d55715945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00, 14.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"/content/content/Spanish_v5\"\n",
        "\n",
        "total_pg = []\n",
        "total_st = []\n",
        "\n",
        "for filename in tqdm(os.listdir(directory)):\n",
        "    if filename.endswith('.json'):\n",
        "        scores_pg = []\n",
        "        scores_st = []\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for k in data.keys():\n",
        "              for level in data[k].keys():\n",
        "                for prompt in data[k][level]:\n",
        "                  unique_id = md5_hash(prompt['prompt'] + prompt['label'])\n",
        "\n",
        "                  pred = id_to_pred_gpt_4o[unique_id]\n",
        "\n",
        "                  score = best_subspan_score(prompt['label'], pred)\n",
        "\n",
        "                  assert level in [\"paragraph_level\", \"sentence_level\"]\n",
        "                  if level == \"paragraph_level\":\n",
        "                    scores_pg.append(score)\n",
        "                  else:\n",
        "                    scores_st.append(score)\n",
        "        print(f\"{filename}: pg_level: {sum(scores_pg)/len(scores_pg)}, st_level: {sum(scores_st)/len(scores_st)}\")\n",
        "        total_pg.extend(scores_pg)\n",
        "        total_st.extend(scores_st)\n",
        "\n",
        "\n",
        "print(\"GPT-4o Spanish\")\n",
        "\n",
        "\n",
        "print()\n",
        "print(f\"Spanish total: pg_level: {sum(total_pg)/len(total_pg)}, st_level: {sum(total_st)/len(total_st)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsvwiFNouve1",
        "outputId": "41fd3a53-d023-4e40-b256-810fe1d407ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 81.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prec%C3%A1lculo-2ed.json: pg_level: 0.36332958380202474, st_level: 0.29112081513828236\n",
            "c%C3%A1lculo-volumen-2.json: pg_level: 0.22962962962962963, st_level: 0.21212121212121213\n",
            "introducci%C3%B3n-estad%C3%ADstica-empresarial.json: pg_level: 0.42, st_level: 0.34782608695652173\n",
            "qu%C3%ADmica-2ed.json: pg_level: 0.5601851851851852, st_level: 0.46494464944649444\n",
            "c%C3%A1lculo-volumen-3.json: pg_level: 0.3201219512195122, st_level: 0.27530364372469635\n",
            "f%C3%ADsica-universitaria-volumen-3.json: pg_level: 0.4463373083475298, st_level: 0.36429872495446264\n",
            "c%C3%A1lculo-volumen-1.json: pg_level: 0.38267148014440433, st_level: 0.27149321266968324\n",
            "f%C3%ADsica-universitaria-volumen-2.json: pg_level: 0.4709193245778612, st_level: 0.3617886178861789\n",
            "f%C3%ADsica-universitaria-volumen-1.json: pg_level: 0.41569282136894825, st_level: 0.30141843971631205\n",
            "introducci%C3%B3n-estad%C3%ADstica.json: pg_level: 0.3739495798319328, st_level: 0.3036649214659686\n",
            "GPT-4o Spanish\n",
            "\n",
            "Spanish total: pg_level: 0.4268131868131868, st_level: 0.34511698880976605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"/content/content/Polish_v5\"\n",
        "\n",
        "total_pg = []\n",
        "total_st = []\n",
        "\n",
        "for filename in tqdm(os.listdir(directory)):\n",
        "    if filename.endswith('.json'):\n",
        "        scores_pg = []\n",
        "        scores_st = []\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for k in data.keys():\n",
        "              for level in data[k].keys():\n",
        "                for prompt in data[k][level]:\n",
        "                  unique_id = md5_hash(prompt['prompt'] + prompt['label'])\n",
        "\n",
        "                  pred = id_to_pred_gpt_4o[unique_id]\n",
        "\n",
        "                  score = best_subspan_score(prompt['label'], pred)\n",
        "\n",
        "                  assert level in [\"paragraph_level\", \"sentence_level\"]\n",
        "                  if level == \"paragraph_level\":\n",
        "                    scores_pg.append(score)\n",
        "                  else:\n",
        "                    scores_st.append(score)\n",
        "        print(f\"{filename}: pg_level: {sum(scores_pg)/len(scores_pg)}, st_level: {sum(scores_st)/len(scores_st)}\")\n",
        "        total_pg.extend(scores_pg)\n",
        "        total_st.extend(scores_st)\n",
        "\n",
        "\n",
        "print(\"GPT-4o Polish\")\n",
        "\n",
        "\n",
        "print()\n",
        "print(f\"Spanish total: pg_level: {sum(total_pg)/len(total_pg)}, st_level: {sum(total_st)/len(total_st)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHUsMBvdu0LE",
        "outputId": "3518da21-e030-47c9-89be-03e8dfa78816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/6 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fizyka-dla-szk%C3%B3%C5%82-wy%C5%BCszych-tom-3.json: pg_level: 0.39195979899497485, st_level: 0.30782918149466193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 2/6 [00:00<00:00, 10.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "psychologia-polska.json: pg_level: 0.4276729559748428, st_level: 0.30428863172226006\n",
            "fizyka-dla-szk%C3%B3%C5%82-wy%C5%BCszych-tom-2.json: pg_level: 0.3526315789473684, st_level: 0.2574626865671642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00, 16.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mikroekonomia-podstawy.json: pg_level: 0.3569553805774278, st_level: 0.26525198938992045\n",
            "fizyka-dla-szk%C3%B3%C5%82-wy%C5%BCszych-tom-1.json: pg_level: 0.32666666666666666, st_level: 0.2202486678507993\n",
            "makroekonomia-podstawy.json: pg_level: 0.3502202643171806, st_level: 0.25669642857142855\n",
            "GPT-4o Polish\n",
            "\n",
            "Spanish total: pg_level: 0.38311068702290074, st_level: 0.27737041719342603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VseNP84Uu3i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-4-turbo"
      ],
      "metadata": {
        "id": "eFALEFrKvHbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts =[]\n",
        "\n",
        "for data in client.batches.list():\n",
        "  data_dict = data.to_dict()\n",
        "  if data_dict['status'] not in ['failed', 'cancelled', 'cancelling'] and 'gpt-4-turbo-multilingual' in data_dict['metadata']['description']:\n",
        "\n",
        "    file_response = client.files.content(data_dict['output_file_id'])\n",
        "\n",
        "    texts.append(file_response.text)"
      ],
      "metadata": {
        "id": "AY1D9SMXvI-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4sJo_vWvTaq",
        "outputId": "7e4c8bdd-37f3-467e-c7b7-9bb7edb75f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts =[]\n",
        "\n",
        "for data in client.batches.list():\n",
        "  data_dict = data.to_dict()\n",
        "  if data_dict['id'] in ['batch_670d9e926f5c81909e9f825ff353f288', 'batch_670da35951688190a9fc100a07d1d825', 'batch_670daad8a3648190ae6813753abe529a', 'batch_670db2da674c81909e3a91d9f9520e90', 'batch_670dbbdb06108190bdecc247db191eef']:\n",
        "\n",
        "    file_response = client.files.content(data_dict['output_file_id'])\n",
        "\n",
        "    texts.append(file_response.text)"
      ],
      "metadata": {
        "id": "0d_eFliX5Mjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uvf9qgUO5pVK",
        "outputId": "4ba59387-3c51-4b27-f8a1-2f0ac174fb87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "id_to_pred_gpt_4_turbo = {}\n",
        "\n",
        "for text in tqdm(texts):\n",
        "  lines = text.split('\\n')\n",
        "\n",
        "  for line in lines[:-1]:\n",
        "    x = json.loads(line)\n",
        "    id_to_pred_gpt_4_turbo[x['custom_id'].split('_')[-1]] = x['response']['body']['choices'][0]['message']['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUrQtEwnvKdd",
        "outputId": "025c662d-276e-408f-c543-109ff116cfb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 24.34it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text in tqdm(texts):\n",
        "  lines = text.split('\\n')\n",
        "\n",
        "  for line in lines[:-1]:\n",
        "    x = json.loads(line)\n",
        "    if x['custom_id'].split('_')[-1] not in id_to_pred_gpt_4_turbo.keys():\n",
        "      id_to_pred_gpt_4_turbo[x['custom_id'].split('_')[-1]] = x['response']['body']['choices'][0]['message']['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYnE8cWivx_9",
        "outputId": "3361b7fa-0c9b-4128-a8b3-8a1b331bff81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 26.39it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"/content/content/Spanish_v5\"\n",
        "\n",
        "total_pg = []\n",
        "total_st = []\n",
        "\n",
        "for filename in tqdm(os.listdir(directory)):\n",
        "    if filename.endswith('.json'):\n",
        "        scores_pg = []\n",
        "        scores_st = []\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for k in data.keys():\n",
        "              for level in data[k].keys():\n",
        "                for prompt in data[k][level]:\n",
        "                  unique_id = md5_hash(prompt['prompt'] + prompt['label'])\n",
        "\n",
        "                  pred = id_to_pred_gpt_4_turbo[unique_id]\n",
        "\n",
        "                  score = best_subspan_score(prompt['label'], pred)\n",
        "\n",
        "                  assert level in [\"paragraph_level\", \"sentence_level\"]\n",
        "                  if level == \"paragraph_level\":\n",
        "                    scores_pg.append(score)\n",
        "                  else:\n",
        "                    scores_st.append(score)\n",
        "        print(f\"{filename}: pg_level: {sum(scores_pg)/len(scores_pg)}, st_level: {sum(scores_st)/len(scores_st)}\")\n",
        "        total_pg.extend(scores_pg)\n",
        "        total_st.extend(scores_st)\n",
        "\n",
        "\n",
        "print(\"GPT-4-turbo Spanish\")\n",
        "\n",
        "\n",
        "print()\n",
        "print(f\"Spanish total: pg_level: {sum(total_pg)/len(total_pg)}, st_level: {sum(total_st)/len(total_st)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRMCL6OovPYk",
        "outputId": "1926746d-d138-40a3-9c38-4ca5ec74c17a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 108.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prec%C3%A1lculo-2ed.json: pg_level: 0.3734533183352081, st_level: 0.33478893740902477\n",
            "c%C3%A1lculo-volumen-2.json: pg_level: 0.26666666666666666, st_level: 0.24242424242424243\n",
            "introducci%C3%B3n-estad%C3%ADstica-empresarial.json: pg_level: 0.42, st_level: 0.42028985507246375\n",
            "qu%C3%ADmica-2ed.json: pg_level: 0.5648148148148148, st_level: 0.46371463714637146\n",
            "c%C3%A1lculo-volumen-3.json: pg_level: 0.3384146341463415, st_level: 0.2550607287449393\n",
            "f%C3%ADsica-universitaria-volumen-3.json: pg_level: 0.4616695059625213, st_level: 0.3806921675774135\n",
            "c%C3%A1lculo-volumen-1.json: pg_level: 0.37906137184115524, st_level: 0.29411764705882354\n",
            "f%C3%ADsica-universitaria-volumen-2.json: pg_level: 0.46716697936210133, st_level: 0.3678861788617886\n",
            "f%C3%ADsica-universitaria-volumen-1.json: pg_level: 0.4040066777963272, st_level: 0.3067375886524823\n",
            "introducci%C3%B3n-estad%C3%ADstica.json: pg_level: 0.42016806722689076, st_level: 0.3036649214659686\n",
            "GPT-4-turbo Spanish\n",
            "\n",
            "Spanish total: pg_level: 0.4342857142857143, st_level: 0.3583418107833164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"/content/content/Polish_v5\"\n",
        "\n",
        "total_pg = []\n",
        "total_st = []\n",
        "\n",
        "for filename in tqdm(os.listdir(directory)):\n",
        "    if filename.endswith('.json'):\n",
        "        scores_pg = []\n",
        "        scores_st = []\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for k in data.keys():\n",
        "              for level in data[k].keys():\n",
        "                for prompt in data[k][level]:\n",
        "                  unique_id = md5_hash(prompt['prompt'] + prompt['label'])\n",
        "\n",
        "                  pred = id_to_pred_gpt_4_turbo[unique_id]\n",
        "\n",
        "                  score = best_subspan_score(prompt['label'], pred)\n",
        "\n",
        "                  assert level in [\"paragraph_level\", \"sentence_level\"]\n",
        "                  if level == \"paragraph_level\":\n",
        "                    scores_pg.append(score)\n",
        "                  else:\n",
        "                    scores_st.append(score)\n",
        "        print(f\"{filename}: pg_level: {sum(scores_pg)/len(scores_pg)}, st_level: {sum(scores_st)/len(scores_st)}\")\n",
        "        total_pg.extend(scores_pg)\n",
        "        total_st.extend(scores_st)\n",
        "\n",
        "\n",
        "print(\"GPT-4-turbo Polish\")\n",
        "\n",
        "\n",
        "print()\n",
        "print(f\"Spanish total: pg_level: {sum(total_pg)/len(total_pg)}, st_level: {sum(total_st)/len(total_st)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5ne1IY-vcFl",
        "outputId": "5f2f0e61-48d2-4009-f4f2-ca5a14347b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00, 35.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fizyka-dla-szk%C3%B3%C5%82-wy%C5%BCszych-tom-3.json: pg_level: 0.3065326633165829, st_level: 0.26868327402135234\n",
            "psychologia-polska.json: pg_level: 0.32767295597484275, st_level: 0.23281143635125937\n",
            "fizyka-dla-szk%C3%B3%C5%82-wy%C5%BCszych-tom-2.json: pg_level: 0.30701754385964913, st_level: 0.21828358208955223\n",
            "mikroekonomia-podstawy.json: pg_level: 0.33070866141732286, st_level: 0.22015915119363394\n",
            "fizyka-dla-szk%C3%B3%C5%82-wy%C5%BCszych-tom-1.json: pg_level: 0.295, st_level: 0.19715808170515098\n",
            "makroekonomia-podstawy.json: pg_level: 0.28634361233480177, st_level: 0.22321428571428573\n",
            "GPT-4-turbo Polish\n",
            "\n",
            "Spanish total: pg_level: 0.31297709923664124, st_level: 0.22857142857142856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Llama 3.1"
      ],
      "metadata": {
        "id": "pBTkbdiG78vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/id_to_pred_spanish.json', 'r') as f:\n",
        "  id_to_pred_llama_spanish = json.load(f)"
      ],
      "metadata": {
        "id": "Qj329VjTwNF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"/content/content/Spanish_v5\"\n",
        "\n",
        "total_pg = []\n",
        "total_st = []\n",
        "\n",
        "for filename in tqdm(os.listdir(directory)):\n",
        "    if filename.endswith('.json'):\n",
        "        scores_pg = []\n",
        "        scores_st = []\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for k in data.keys():\n",
        "              for level in data[k].keys():\n",
        "                for prompt in data[k][level]:\n",
        "                  unique_id = md5_hash(prompt['prompt'] + prompt['label'])\n",
        "\n",
        "                  pred = id_to_pred_llama_spanish[unique_id]\n",
        "\n",
        "                  score = best_subspan_score(prompt['label'], pred)\n",
        "\n",
        "                  assert level in [\"paragraph_level\", \"sentence_level\"]\n",
        "                  if level == \"paragraph_level\":\n",
        "                    scores_pg.append(score)\n",
        "                  else:\n",
        "                    scores_st.append(score)\n",
        "        print(f\"{filename}: pg_level: {sum(scores_pg)/len(scores_pg)}, st_level: {sum(scores_st)/len(scores_st)}\")\n",
        "        total_pg.extend(scores_pg)\n",
        "        total_st.extend(scores_st)\n",
        "\n",
        "\n",
        "print(\"Llama 3.1 Spanish\")\n",
        "\n",
        "\n",
        "print()\n",
        "print(f\"Spanish total: pg_level: {sum(total_pg)/len(total_pg)}, st_level: {sum(total_st)/len(total_st)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54eidqhJ8C-1",
        "outputId": "845acf8c-8a67-4932-b1d8-680ab41d50ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 108.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prec%C3%A1lculo-2ed.json: pg_level: 0.4881889763779528, st_level: 0.43377001455604075\n",
            "c%C3%A1lculo-volumen-2.json: pg_level: 0.362962962962963, st_level: 0.2727272727272727\n",
            "introducci%C3%B3n-estad%C3%ADstica-empresarial.json: pg_level: 0.57, st_level: 0.4782608695652174\n",
            "qu%C3%ADmica-2ed.json: pg_level: 0.6354166666666666, st_level: 0.5313653136531366\n",
            "c%C3%A1lculo-volumen-3.json: pg_level: 0.4146341463414634, st_level: 0.3157894736842105\n",
            "f%C3%ADsica-universitaria-volumen-3.json: pg_level: 0.5178875638841567, st_level: 0.4426229508196721\n",
            "c%C3%A1lculo-volumen-1.json: pg_level: 0.4657039711191336, st_level: 0.3031674208144796\n",
            "f%C3%ADsica-universitaria-volumen-2.json: pg_level: 0.551594746716698, st_level: 0.4491869918699187\n",
            "f%C3%ADsica-universitaria-volumen-1.json: pg_level: 0.44741235392320533, st_level: 0.3475177304964539\n",
            "introducci%C3%B3n-estad%C3%ADstica.json: pg_level: 0.49159663865546216, st_level: 0.3717277486910995\n",
            "Llama 3.1 Spanish\n",
            "\n",
            "Spanish total: pg_level: 0.5136263736263736, st_level: 0.4237029501525941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/id_to_pred_polish.json', 'r') as f:\n",
        "  id_to_pred_llama_polish = json.load(f)"
      ],
      "metadata": {
        "id": "Qipp2tbc8HT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"/content/content/Polish_v5\"\n",
        "\n",
        "total_pg = []\n",
        "total_st = []\n",
        "\n",
        "for filename in tqdm(os.listdir(directory)):\n",
        "    if filename.endswith('.json'):\n",
        "        scores_pg = []\n",
        "        scores_st = []\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for k in data.keys():\n",
        "              for level in data[k].keys():\n",
        "                for prompt in data[k][level]:\n",
        "                  unique_id = md5_hash(prompt['prompt'] + prompt['label'])\n",
        "\n",
        "                  pred = id_to_pred_llama_polish[unique_id]\n",
        "\n",
        "                  score = best_subspan_score(prompt['label'], pred)\n",
        "\n",
        "                  assert level in [\"paragraph_level\", \"sentence_level\"]\n",
        "                  if level == \"paragraph_level\":\n",
        "                    scores_pg.append(score)\n",
        "                  else:\n",
        "                    scores_st.append(score)\n",
        "        print(f\"{filename}: pg_level: {sum(scores_pg)/len(scores_pg)}, st_level: {sum(scores_st)/len(scores_st)}\")\n",
        "        total_pg.extend(scores_pg)\n",
        "        total_st.extend(scores_st)\n",
        "\n",
        "\n",
        "print(\"Llama 3.1 Polish\")\n",
        "\n",
        "\n",
        "print()\n",
        "print(f\"Polish total: pg_level: {sum(total_pg)/len(total_pg)}, st_level: {sum(total_st)/len(total_st)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ANKBHom8Jml",
        "outputId": "969f5b81-41ca-408f-9875-91f401e80625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00, 27.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fizyka-dla-szk%C3%B3%C5%82-wy%C5%BCszych-tom-3.json: pg_level: 0.4204355108877722, st_level: 0.30782918149466193\n",
            "psychologia-polska.json: pg_level: 0.4, st_level: 0.28727025187202176\n",
            "fizyka-dla-szk%C3%B3%C5%82-wy%C5%BCszych-tom-2.json: pg_level: 0.4, st_level: 0.292910447761194\n",
            "mikroekonomia-podstawy.json: pg_level: 0.41732283464566927, st_level: 0.2493368700265252\n",
            "fizyka-dla-szk%C3%B3%C5%82-wy%C5%BCszych-tom-1.json: pg_level: 0.3616666666666667, st_level: 0.23268206039076378\n",
            "makroekonomia-podstawy.json: pg_level: 0.3788546255506608, st_level: 0.2700892857142857\n",
            "Llama 3.1 Polish\n",
            "\n",
            "Polish total: pg_level: 0.3967080152671756, st_level: 0.27762326169405815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SP7YzOSy8NaU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}